{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hacim tahminleme\n",
    "    - klasik model\n",
    "        - diğer featurelar\n",
    "          \n",
    "                - order day/week/month\n",
    "                - sipariş verme sıklığı\n",
    "                - daily_average\n",
    "                - ürün bazlı sipariş verme sıklığı\n",
    "                - day diff between last order\n",
    "    - past data based model\n",
    "        - convert sales to daily basis\n",
    "        - geçmiş 10/20/30/40/50/60/90 gün verisi\n",
    "    - rnn\n",
    "    - lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trials\n",
    "    - sum of past data\n",
    "    - average of past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = pd.read_csv('order_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_date</th>\n",
       "      <th>material_num</th>\n",
       "      <th>customer_num</th>\n",
       "      <th>order_item</th>\n",
       "      <th>order_type</th>\n",
       "      <th>order_amount</th>\n",
       "      <th>palette_std</th>\n",
       "      <th>operation_type</th>\n",
       "      <th>order_num</th>\n",
       "      <th>order_weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>material_last_10_day_sum</th>\n",
       "      <th>material_last_14_day_sum</th>\n",
       "      <th>material_last_30_day_sum</th>\n",
       "      <th>whole_last_2_day_sum</th>\n",
       "      <th>whole_last_3_day_sum</th>\n",
       "      <th>whole_last_5_day_sum</th>\n",
       "      <th>whole_last_7_day_sum</th>\n",
       "      <th>whole_last_10_day_sum</th>\n",
       "      <th>whole_last_14_day_sum</th>\n",
       "      <th>whole_last_30_day_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>91704</td>\n",
       "      <td>M10227</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>O2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>91704</td>\n",
       "      <td>M10229</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>O2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>91704</td>\n",
       "      <td>M10232</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>O2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>91704</td>\n",
       "      <td>M10234</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>O2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>91704</td>\n",
       "      <td>M10235</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>O2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_date material_num customer_num  order_item  order_type  order_amount  \\\n",
       "0  2020-01-02        91704       M10227           1        20.0             1   \n",
       "1  2020-01-02        91704       M10229           1        20.0             2   \n",
       "2  2020-01-02        91704       M10232           1        20.0             2   \n",
       "3  2020-01-02        91704       M10234           1        20.0             1   \n",
       "4  2020-01-02        91704       M10235           1        20.0             3   \n",
       "\n",
       "   palette_std operation_type  order_num  order_weekday  ...  \\\n",
       "0         80.0             O2          1              3  ...   \n",
       "1         80.0             O2          1              3  ...   \n",
       "2         80.0             O2          1              3  ...   \n",
       "3         80.0             O2          1              3  ...   \n",
       "4         80.0             O2          1              3  ...   \n",
       "\n",
       "   material_last_10_day_sum  material_last_14_day_sum  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   material_last_30_day_sum  whole_last_2_day_sum  whole_last_3_day_sum  \\\n",
       "0                       0.0                   0.0                   0.0   \n",
       "1                       0.0                   0.0                   0.0   \n",
       "2                       0.0                   0.0                   0.0   \n",
       "3                       0.0                   0.0                   0.0   \n",
       "4                       0.0                   0.0                   0.0   \n",
       "\n",
       "   whole_last_5_day_sum  whole_last_7_day_sum  whole_last_10_day_sum  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   0.0                   0.0                    0.0   \n",
       "2                   0.0                   0.0                    0.0   \n",
       "3                   0.0                   0.0                    0.0   \n",
       "4                   0.0                   0.0                    0.0   \n",
       "\n",
       "   whole_last_14_day_sum  whole_last_30_day_sum  \n",
       "0                    0.0                    0.0  \n",
       "1                    0.0                    0.0  \n",
       "2                    0.0                    0.0  \n",
       "3                    0.0                    0.0  \n",
       "4                    0.0                    0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_date                   object\n",
       "material_num                 object\n",
       "customer_num                 object\n",
       "order_item                    int64\n",
       "order_type                  float64\n",
       "order_amount                  int64\n",
       "palette_std                 float64\n",
       "operation_type               object\n",
       "order_num                     int64\n",
       "order_weekday                 int64\n",
       "order_month                   int64\n",
       "order_day                     int64\n",
       "order_week                    int64\n",
       "cust_mat_last_2_day_sum     float64\n",
       "cust_mat_last_3_day_sum     float64\n",
       "cust_mat_last_5_day_sum     float64\n",
       "cust_mat_last_7_day_sum     float64\n",
       "cust_mat_last_10_day_sum    float64\n",
       "cust_mat_last_14_day_sum    float64\n",
       "cust_mat_last_30_day_sum    float64\n",
       "customer_last_2_day_sum     float64\n",
       "customer_last_3_day_sum     float64\n",
       "customer_last_5_day_sum     float64\n",
       "customer_last_7_day_sum     float64\n",
       "customer_last_10_day_sum    float64\n",
       "customer_last_14_day_sum    float64\n",
       "customer_last_30_day_sum    float64\n",
       "material_last_2_day_sum     float64\n",
       "material_last_3_day_sum     float64\n",
       "material_last_5_day_sum     float64\n",
       "material_last_7_day_sum     float64\n",
       "material_last_10_day_sum    float64\n",
       "material_last_14_day_sum    float64\n",
       "material_last_30_day_sum    float64\n",
       "whole_last_2_day_sum        float64\n",
       "whole_last_3_day_sum        float64\n",
       "whole_last_5_day_sum        float64\n",
       "whole_last_7_day_sum        float64\n",
       "whole_last_10_day_sum       float64\n",
       "whole_last_14_day_sum       float64\n",
       "whole_last_30_day_sum       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates20 = pd.DataFrame(pd.date_range(start='01-01-2020', end='31-12-2020'), columns=['date'])\n",
    "dates21 = pd.DataFrame(pd.date_range(start='01-01-2021', end='30-11-2021'), columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_20 = pd.read_excel('additional_data/currency_20.xlsx', parse_dates=['date'])\n",
    "currency_20.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_21 = pd.read_excel('additional_data/currency_21.xlsx', parse_dates=['date'])\n",
    "currency_21.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_20 = pd.read_excel('additional_data/holidays_20.xlsx', parse_dates=['date'])\n",
    "holidays_20 = pd.merge(dates20, holidays_20, on=['date'], how='left')\n",
    "holidays_20.fillna(1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_21 = pd.read_excel('additional_data/holidays_21.xlsx', parse_dates=['date'])\n",
    "holidays_21 = pd.merge(dates21, holidays_21, on=['date'], how='left')\n",
    "holidays_21.fillna(1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tufe_20 = pd.read_excel('additional_data/tufe_20.xlsx', parse_dates=['date'])\n",
    "tufe_20 = pd.merge(dates20, tufe_20, on=['date'], how='left')\n",
    "tufe_20.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tufe_21 = pd.read_excel('additional_data/tufe_21.xlsx', parse_dates=['date'])\n",
    "tufe_21 = pd.merge(dates21, tufe_21, on=['date'], how='left')\n",
    "tufe_21.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufe_20 = pd.read_excel('additional_data/ufe_20.xlsx', parse_dates=['date'])\n",
    "ufe_20 = pd.merge(dates20, ufe_20, on=['date'], how='left')\n",
    "ufe_20.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufe_21 = pd.read_excel('additional_data/ufe_21.xlsx', parse_dates=['date'])\n",
    "ufe_21 = pd.merge(dates21, ufe_21, on=['date'], how='left')\n",
    "ufe_21.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-90a983e8b151>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-90a983e8b151>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tüfe ve üfe için artış oranını hesapla\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tüfe ve üfe için artış oranını hesapla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_20 = pd.read_excel('additional_data/holidays_20.xlsx', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_20 = pd.merge(dates20, holidays_20, on=['date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>workday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43831</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43952</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43953</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43955</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44021</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44023</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44132</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date  workday\n",
       "0   43831      0.0\n",
       "1   43944      0.0\n",
       "2   43952      0.0\n",
       "3   43953      0.5\n",
       "4   43954      0.0\n",
       "5   43955      0.0\n",
       "6   43956      0.0\n",
       "7   43970      0.0\n",
       "8   44021      0.5\n",
       "9   44022      0.0\n",
       "10  44023      0.0\n",
       "11  44024      0.0\n",
       "12  44025      0.0\n",
       "13  44027      0.0\n",
       "14  44073      0.0\n",
       "15  44132      0.5\n",
       "16  44133      0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holidays_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df ile birleştir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['order_date', 'order_item', 'order_type', 'operation_type', 'palette_std','order_num']\n",
    "categorical = ['material_num', 'customer_num','order_weekday', 'order_month', 'order_day', 'order_week']\n",
    "numerical = ['cust_mat_last_2_day_sum', 'cust_mat_last_3_day_sum', 'cust_mat_last_5_day_sum', 'cust_mat_last_7_day_sum',\n",
    "               'cust_mat_last_10_day_sum', 'cust_mat_last_14_day_sum', 'cust_mat_last_30_day_sum', 'customer_last_2_day_sum',\n",
    "               'customer_last_3_day_sum', 'customer_last_5_day_sum', 'customer_last_7_day_sum', 'customer_last_10_day_sum',\n",
    "               'customer_last_14_day_sum', 'customer_last_30_day_sum', 'material_last_2_day_sum', 'material_last_3_day_sum',\n",
    "               'material_last_5_day_sum', 'material_last_7_day_sum', 'material_last_10_day_sum', 'material_last_14_day_sum',\n",
    "               'material_last_30_day_sum', 'whole_last_2_day_sum', 'whole_last_3_day_sum', 'whole_last_5_day_sum', \n",
    "               'whole_last_7_day_sum', 'whole_last_10_day_sum', 'whole_last_14_day_sum', 'whole_last_30_day_sum']\n",
    "label = ['order_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_defined_columns = drops + categorical + numerical + label\n",
    "if sorted(order_df.columns) != (sorted(all_defined_columns)):\n",
    "    assert('Columns are not equal!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1 - only past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_transofmer():\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "                                    transformers=[('num', numeric_transformer, numerical),\n",
    "                                                  ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(regressor):\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "                                    transformers=[('num', numeric_transformer, numerical),\n",
    "                                                  ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 regressor])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = order_df[categorical+numerical]\n",
    "y = order_df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series split\n",
    "ts_cv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Train rmse: 35.039590939635616, r2: 0.9080533917582567\n",
      "Test rmse: 153.9198950819373, r2: 0.12702812198055924\n",
      "-------------\n",
      "Train rmse: 50.80712883446026, r2: 0.8726952246293623\n",
      "Test rmse: 134.36036929021452, r2: 0.059694804166442905\n",
      "-------------\n",
      "Train rmse: 57.15203503083463, r2: 0.8360294798215107\n",
      "Test rmse: 147.28847170187905, r2: -0.036837066699342724\n",
      "-------------\n",
      "Train rmse: 59.75815122299565, r2: 0.8229999617524116\n",
      "Test rmse: 127.19895075727185, r2: 0.15608390420059337\n",
      "-------------\n",
      "Train rmse: 62.11589856197317, r2: 0.8068694576732087\n",
      "Test rmse: 210.77855389662207, r2: 0.37243519911355827\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in ts_cv.split(X):\n",
    "\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    print('-------------')\n",
    "    pipeline = create_pipeline(('regressor', XGBRegressor(gamma = 0.05, learning_rate= 0.1, max_depth=5,\n",
    "                                                     n_estimators= 1000, n_jobs= 8, objective= 'reg:squarederror',\n",
    "                                                     reg_alpha= 0.5, reg_lambda= 1, scale_pos_weight=1, subsample= 1.0)))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    test_preds = pipeline.predict(X_test)\n",
    "    print(f'Train rmse: {mean_squared_error(y_train, train_preds, squared=False)}, r2: {r2_score(y_train, train_preds)}')\n",
    "    print(f'Test rmse: {mean_squared_error(y_test, test_preds, squared=False)}, r2: {r2_score(y_test, test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in ts_cv.split(X):\n",
    "\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    print('-------------')\n",
    "    pipeline = create_pipeline(('regressor', HistGradientBoostingRegressor()))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    test_preds = pipeline.predict(X_test)\n",
    "    print(f'Train rmse: {mean_squared_error(y_train, train_preds, squared=False)}, r2: {r2_score(y_train, train_preds)}')\n",
    "    print(f'Test rmse: {mean_squared_error(y_test, test_preds, squared=False)}, r2: {r2_score(y_test, test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.loc[:359824]\n",
    "y_train = y.loc[:359824]\n",
    "X_test = X.loc[359824:]\n",
    "y_test = y.loc[359824:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_pipe = create_pipeline_transofmer()\n",
    "X_train = trans_pipe.fit_transform(X_train).toarray()\n",
    "X_test = trans_pipe.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359825, 1502)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_num\n",
       "C-10005    880903\n",
       "C-10053    509023\n",
       "M10015     380185\n",
       "M10138     362374\n",
       "M10117     292622\n",
       "M10387     265173\n",
       "C-10051    202505\n",
       "C-10089    194244\n",
       "M10235     181587\n",
       "C-10110    169788\n",
       "Name: order_amount, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.groupby(['customer_num'])['order_amount'].sum().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_num\n",
       "C-10001    16635\n",
       "C-10012    14403\n",
       "C-10028    13998\n",
       "C-10029    12147\n",
       "C-10030     9077\n",
       "C-10069     4982\n",
       "M10235      4623\n",
       "M10227      4524\n",
       "M10231      4472\n",
       "M10230      4428\n",
       "Name: order_amount, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.groupby(['customer_num'])['order_amount'].count().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-77b79f53ac13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# model.add(Dropout(0.2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, return_sequences, return_state, go_backwards, stateful, time_major, unroll, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_runtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_runtime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m     super(LSTM, self).__init__(\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_non_trackable_mask_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropoutRNNCellMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[0;32m   2767\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trainable'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2768\u001b[0m         **cell_kwargs)\n\u001b[1;32m-> 2769\u001b[1;33m     super(LSTM, self).__init__(\n\u001b[0m\u001b[0;32m   2770\u001b[0m         \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2771\u001b[0m         \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, time_major, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_shape'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[0mbatch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=80, return_sequences=True,activation='relu', input_shape=(X_train.shape[1])))\n",
    "model.add(LSTM(units=80, return_sequences=True,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=80, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split = 0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(431788, 41)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2 - use other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series split\n",
    "ts_cv = TimeSeriesSplit(\n",
    "         n_splits=5,  #Number of splits used\n",
    "         gap=0,  #No time needed between sets\n",
    "         max_train_size=None, #Auto train sample size \n",
    "         test_size=None, #Auto test sample size)\n",
    "    \n",
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "    )\n",
    "    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "    print(\n",
    "        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n",
    "        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_pipeline = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"categorical\", ordinal_encoder, categorical_columns),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    HistGradientBoostingRegressor(\n",
    "        categorical_features=range(2),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gbrt_pipeline, X, y, cv=ts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-6, 6, 25)\n",
    "naive_linear_pipeline = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"categorical\", one_hot_encoder, categorical_columns),\n",
    "        ],\n",
    "        remainder=MinMaxScaler(),\n",
    "    ),\n",
    "    RidgeCV(alphas=alphas),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(naive_linear_pipeline, X, y, cv=ts_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sin and cos transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_df = pd.DataFrame(\n",
    "    np.arange(26).reshape(-1, 1),\n",
    "    columns=[\"seasons\"],\n",
    ")\n",
    "seasons_df[\"seasons_sin\"] = sin_transformer(4).fit_transform(seasons_df)[\"seasons\"]\n",
    "seasons_df[\"seasons_cos\"] = cos_transformer(4).fit_transform(seasons_df)[\"seasons\"]\n",
    "seasons_df.plot(x=\"seasons\")_ = plt.title(\"Trigonometric encoding for the 'seasons' feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclic_cossin_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", one_hot_encoder, categorical_columns),\n",
    "        (\"seasons_sin\", sin_transformer(4), [\"seasons\"]),\n",
    "        (\"seasons_cos\", cos_transformer(4), [\"seasons\"]),\n",
    "    ],\n",
    "    remainder=MinMaxScaler(),\n",
    ")\n",
    "cyclic_cossin_linear_pipeline = make_pipeline(\n",
    "    cyclic_cossin_transformer,\n",
    "    RidgeCV(alphas=alphas),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cyclic_cossin_linear_pipeline, X, y, cv=ts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for transformation from time series to supervised\n",
    "df_supervised = df_diff.drop(['prev_sales'],axis=1)\n",
    "#adding lags\n",
    "for inc in range(1,13):\n",
    "    field_name = 'lag_' + str(inc)\n",
    "    df_supervised[field_name] = df_supervised['diff'].shift(inc)\n",
    "#drop null values\n",
    "df_supervised = df_supervised.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols regression\n",
    "\n",
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf\n",
    "# Define the regression formula\n",
    "model = smf.ols(formula='diff ~ lag_1', data=df_supervised)\n",
    "# Fit the regression\n",
    "model_fit = model.fit()\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = model_fit.rsquared_adj\n",
    "print(regression_adj_rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### day difference between last order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_order_dates = df.groupby(['customer_num','order_num', 'material_num'])['order_date'].min().reset_index()\n",
    "material_order_dates['shifted_date'] = material_order_dates.groupby(['customer_num','order_num', 'material_num'])['order_date'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last 1,2,3,5,7,10 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_amounts = df.groupby(['customer_num', 'material_num','order_num'])['order_amount'].sum().reset_index()\n",
    "for period in [1,2,3,5,7,10]:\n",
    "    col_name = 'last_' + str(period) + '_period_sum'\n",
    "    order_amounts['rolling_sum'] = order_amounts.groupby(['customer_num', 'material_num'])['order_amount'] \\\n",
    "    .rolling(period, min_periods=period).sum().values\n",
    "    \n",
    "    order_amounts[col_name] = order_amounts.groupby(['customer_num', 'material_num'])['rolling_sum'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_amounts.isna().sum()/order_amounts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, order_amounts.drop(['order_amount', 'rolling_sum'], axis=1), on=['customer_num', 'material_num', 'order_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order_item numbers in an order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df.groupby(['order_num'])['order_item'].count().reset_index()\n",
    "item_counts.columns = ['order_num', 'num_order_item']\n",
    "df = pd.merge(df, item_counts, on=['order_num'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last period averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_2_period_avg'] = df['last_2_period_sum']/2\n",
    "df['last_3_period_avg'] = df['last_3_period_sum']/3\n",
    "df['last_5_period_avg'] = df['last_5_period_sum']/5\n",
    "df['last_7_period_avg'] = df['last_7_period_sum']/7\n",
    "df['last_10_period_avg'] = df['last_10_period_sum']/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_resample = df.rolling('7D')\n",
    "aggregated_df = weekly_resample.agg(['min', 'mean', 'max', 'std'])\n",
    "aggregated_df.columns = ['_'.join(col).strip() + '_week' for col in \n",
    "                         aggregated_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.sort_values(by=['order_date','order_num'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(order_df)):\n",
    "    if order_df.order_num[i] > order_df.order_num[i+1]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "order_summary = order_df.groupby(['order_date'])['order_amount'].sum()\n",
    "adf_result = adfuller(order_summary)\n",
    "print(adf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean encoding for customer and product ??? or did we include that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_target_enc_na = .3343 # default na replacement\n",
    "# Expanding Mean\n",
    "cumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\n",
    "cumcnt = all_data.groupby('item_id')['target'].cumcount()\n",
    "\n",
    "all_data['item_target_enc'] = cumsum/cumcnt\n",
    "all_data['item_target_enc'].fillna(item_target_enc_na,inplace=True)\n",
    "corr = np.corrcoef(all_data['target'].values, all_data['item_target_enc'])[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadece difference üzerinden featurelar bul\n",
    "\n",
    "    - past data based model\n",
    "        - convert sales to daily basis\n",
    "        - geçmiş 10/20/30/40/50/60/90 gün verisi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data (Features)\n",
    "X_train = all_data[all_data['date_block_num'] < 33]\n",
    "X_train = X_train.drop(['item_cnt_month'], axis=1)\n",
    "# Valid data (Features)\n",
    "X_valid = all_data[all_data['date_block_num'] == 33]\n",
    "X_valid = X_valid.drop(['item_cnt_month'], axis=1)\n",
    "# Test data (Features)\n",
    "X_test = all_data[all_data['date_block_num'] == 34]\n",
    "X_test = X_test.drop(['item_cnt_month'], axis=1)\n",
    "\n",
    "# Train data (Target values)\n",
    "y_train = all_data[all_data['date_block_num'] < 33]['item_cnt_month']\n",
    "# Valid data (Target values)\n",
    "y_valid = all_data[all_data['date_block_num'] == 33]['item_cnt_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# lgb hyper-parameters\n",
    "params = {'metric': 'rmse',\n",
    "          'num_leaves': 255,\n",
    "          'learning_rate': 0.005,\n",
    "          'feature_fraction': 0.75,\n",
    "          'bagging_fraction': 0.75,\n",
    "          'bagging_freq': 5,\n",
    "          'force_col_wise' : True,\n",
    "          'random_state': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['shop_id', 'city', 'item_category_id', 'category', 'month']\n",
    "\n",
    "# lgb train and valid dataset\n",
    "dtrain = lgb.Dataset(X_train, y_train)\n",
    "dvalid = lgb.Dataset(X_valid, y_valid)\n",
    " \n",
    "# Train LightGBM model\n",
    "lgb_model = lgb.train(params=params,\n",
    "                      train_set=dtrain,\n",
    "                      num_boost_round=1500,\n",
    "                      valid_sets=(dtrain, dvalid),\n",
    "                      early_stopping_rounds=150,\n",
    "                      categorical_feature=cat_features,\n",
    "                      verbose_eval=100)      \n",
    "\n",
    "preds = lgb_model.predict(X_test).clip(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction graph for different models\n",
    "\n",
    "#Create teh predicted values:\n",
    "naive_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\n",
    "naive_linear_predictions = naive_linear_pipeline.predict(X.iloc[test_0])\n",
    "one_hot_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\n",
    "one_hot_linear_predictions = one_hot_linear_pipeline.predict(X.iloc[test_0])\n",
    "cyclic_cossin_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\n",
    "cyclic_cossin_linear_predictions = cyclic_cossin_linear_pipeline.predict(X.iloc[test_0])\n",
    "cyclic_spline_linear_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\n",
    "cyclic_spline_linear_predictions = cyclic_spline_linear_pipeline.predict(X.iloc[test_0])\n",
    "#Change \"_0\" with\"_1\", \"_2\", \"_3\" and \"_4\" for remaining splits\n",
    "#Build the graph:\n",
    "last_days = slice(-100, None)\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "fig.suptitle(\"Predictions by linear models\")\n",
    "ax.plot(\n",
    "    y.iloc[test_0].values[last_days], #Change \"_0\" with 1,2,3 and 4\n",
    "    \"x-\",\n",
    "    alpha=0.2,\n",
    "    label=\"Actual absenteeism in hours \",\n",
    "    color=\"black\",\n",
    ")\n",
    "ax.plot(\n",
    "    naive_linear_predictions[last_days], \n",
    "    \"x-\", \n",
    "    label=\"Ordinal time features\"\n",
    ")\n",
    "ax.plot(\n",
    "    cyclic_cossin_linear_predictions[last_days],\n",
    "    \"x-\",\n",
    "    label=\"Trigonometric time features\",\n",
    ")\n",
    "ax.plot(\n",
    "    cyclic_spline_linear_predictions[last_days],\n",
    "    \"x-\",\n",
    "    label=\"Spline-based time features\",\n",
    ")\n",
    "ax.plot(\n",
    "    one_hot_linear_predictions[last_days],\n",
    "    \"x-\",\n",
    "    label=\"One-hot time features\",\n",
    ")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(12, 4), sharey=True)\n",
    "fig.suptitle(\"Non-linear regression models\")\n",
    "predictions = [\n",
    "    cyclic_cossin_linear_predictions,\n",
    "    cyclic_spline_linear_predictions,\n",
    "    one_hot_linear_predictions,\n",
    "]\n",
    "labels = [\n",
    "    \"cyclic_cossin_linear_predictions\",\n",
    "    \"Splines + polynomial kernel\",\n",
    "    \"Gradient Boosted Trees\",\n",
    "]\n",
    "for ax, pred, label in zip(axes, predictions, labels):\n",
    "    ax.scatter(y.iloc[test_0].values, pred, alpha=0.3, label=label)\n",
    "    ax.plot([0, 0.125], [0, 0.125], \"--\", label=\"Perfect model\")\n",
    "    ax.set(\n",
    "           xlim=(0, 0.125),\n",
    "           ylim=(0, 0.125),\n",
    "           xlabel=\"True absenteeism\",\n",
    "           ylabel=\"Predicted absenteeism\",\n",
    "    )\n",
    "    ax.legend()\n",
    "#Change \"test_0\" with _1, _2, _3 or _4 to obtain graphical visualisations for the other splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling non-linear features 1\n",
    "\n",
    "cyclic_spline_poly_pipeline = make_pipeline(\n",
    "    cyclic_spline_transformer,\n",
    "    Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=0),\n",
    "    RidgeCV(alphas=alphas),\n",
    ")\n",
    "evaluate(cyclic_spline_poly_pipeline, X, y, cv=ts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling non-linear features 2\n",
    "one_hot_poly_pipeline = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"categorical\", one_hot_encoder, categorical_columns),         \n",
    "            (\"one_hot_time\", one_hot_encoder, [\"month_of_absence\", \"day_of_the_week\"]),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=0),\n",
    "    RidgeCV(alphas=alphas),\n",
    ")\n",
    "evaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for all customers, all materials and all dates\n",
    "\n",
    "unq_materials = pd.DataFrame(order_df['material_num'].unique(), columns=['material_num'])\n",
    "unq_materials['key'] = 1\n",
    "unq_customers = pd.DataFrame(order_df['customer_num'].unique(), columns=['customer_num'])\n",
    "unq_customers['key'] = 1\n",
    "dates = pd.DataFrame(pd.date_range(start=raw_df['order_date'].min(), end=raw_df['order_date'].max()), columns=['order_date'])\n",
    "dates['key'] = 1\n",
    "all_data = pd.merge(dates, unq_customers, on=['key'])\n",
    "all_data = pd.merge(all_data, unq_materials, on=['key']).drop('key', axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
